# ğŸ¤– GuÃ­a de Agentes - AIFoundry

> DocumentaciÃ³n alineada con la arquitectura real del proyecto.

---

## ğŸ“– Ãndice

1. [Arquitectura Real](#arquitectura-real)
2. [ScraperAgent â€” Agente Base](#scraperagent--agente-base)
3. [Config por Dominio](#config-por-dominio)
4. [Structured Output](#structured-output)
5. [Tools: Locales + MCP](#tools-locales--mcp)
6. [Memoria Conversacional](#memoria-conversacional)
7. [Crear un Nuevo Dominio](#crear-un-nuevo-dominio)
8. [API REST](#api-rest)
9. [Troubleshooting](#troubleshooting)

---

## Arquitectura Real

AIFoundry usa un **agente Ãºnico genÃ©rico** (`ScraperAgent`) que se configura via JSON por dominio. No hay herencia de clases ni registry â€” cada dominio es un `config.json`.

### Stack TecnolÃ³gico

| Componente | TecnologÃ­a |
|------------|-----------|
| **OrquestaciÃ³n** | LangGraph (`create_agent` de `langchain.agents`) |
| **LLM** | LiteLLM proxy â†’ multi-proveedor (OpenAI, Anthropic, etc.) |
| **Tools MCP** | `langchain-mcp-adapters` (Brave Search, Playwright) |
| **Tools locales** | `simple_scrape_url` (BeautifulSoup + Readability) |
| **API** | FastAPI |
| **Structured Output** | `response_format=` nativo de `create_agent` (Pydantic) |

### Diagrama

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     FastAPI API      â”‚
                    â”‚   (router.py)        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    ScraperAgent      â”‚  â† agente Ãºnico genÃ©rico
                    â”‚   (base/agent.py)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                â”‚                â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ salary/       â”‚  â”‚electricity/â”‚  â”‚social_comments/â”‚
     â”‚ config.json   â”‚  â”‚config.json â”‚  â”‚config.json     â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

     Tools:
     â”œâ”€â”€ brave_web_search (MCP - Docker)
     â”œâ”€â”€ playwright (MCP - Docker)
     â””â”€â”€ simple_scrape_url (local)
```

### Estructura de Archivos

```
aifoundry/app/core/agents/
â”œâ”€â”€ base/
â”‚   â”œâ”€â”€ agent.py          # ScraperAgent â€” agente ReAct genÃ©rico
â”‚   â”œâ”€â”€ config_schema.py  # Schema Pydantic para validar config.json
â”‚   â”œâ”€â”€ prompts.py        # get_system_prompt(config) â€” prompt dinÃ¡mico
â”‚   â””â”€â”€ tools.py          # get_local_tools() â€” simple_scrape_url
â”‚
â”œâ”€â”€ salary/
â”‚   â””â”€â”€ config.json       # Config especÃ­fica de salarios
â”œâ”€â”€ electricity/
â”‚   â””â”€â”€ config.json       # Config especÃ­fica de electricidad
â””â”€â”€ social_comments/
    â””â”€â”€ config.json       # Config especÃ­fica de comentarios sociales
```

---

## ScraperAgent â€” Agente Base

`ScraperAgent` es un agente ReAct que:

1. Recibe una config dict con `product`, `provider`, `country_code`, `query`, etc.
2. Genera un system prompt **dinÃ¡mico** basado en esa config
3. Usa tools MCP (Brave, Playwright) + tools locales para scraping
4. Opcionalmente devuelve structured output via Pydantic

### Uso bÃ¡sico

```python
from aifoundry.app.core.agents.scraper.agent import ScraperAgent

async with ScraperAgent(use_mcp=True) as agent:
    result = await agent.run({
        "product": "electricity",
        "provider": "Endesa",
        "country_code": "ES",
        "language": "es",
        "query": "precio electricidad Endesa EspaÃ±a febrero 2026",
    })
    print(result["output"])
```

### Uso con Structured Output (RECOMENDADO)

```python
from aifoundry.app.core.agents.scraper.agent import ScraperAgent
from aifoundry.app.schemas.agent_responses import SalaryResponse

async with ScraperAgent(
    use_mcp=True,
    response_model=SalaryResponse,  # â† structured output nativo, 1 sola llamada LLM
) as agent:
    result = await agent.run(config)
    structured = result["structured_response"]  # â† objeto Pydantic
```

### ParÃ¡metros del constructor

| ParÃ¡metro | Tipo | Default | DescripciÃ³n |
|-----------|------|---------|-------------|
| `tools` | `List[BaseTool]` | `None` | Tools locales custom. Si None, usa `get_local_tools()` |
| `use_mcp` | `bool` | `True` | Cargar tools MCP (Brave, Playwright) |
| `disable_simple_scrape` | `bool` | `False` | Excluir `simple_scrape_url` |
| `verbose` | `bool` | `True` | Logging en tiempo real via callbacks |
| `use_memory` | `bool` | `True` | Memoria conversacional (InMemorySaver) |
| `response_model` | `Type[BaseModel]` | `None` | **RECOMENDADO**: Pydantic model para structured output nativo |
| `structured_output` | `bool` | `False` | **DEPRECATED**: Usa post-procesamiento (2 llamadas LLM) |
| `agent_name` | `str` | `None` | Nombre para debugging/tracing en LangGraph |

---

## Config por Dominio

Cada dominio tiene un `config.json` con la configuraciÃ³n especÃ­fica:

```json
{
  "product": "salary",
  "query_template": "salario {position} {provider} {city} {country_name} {date}",
  "freshness": "py",
  "extraction_prompt": "Extrae salarios estructurados...",
  "validation_prompt": "Verifica que los datos sean coherentes...",
  "countries": {
    "ES": { "language": "es", "currency": "EUR" },
    "US": { "language": "en", "currency": "USD" }
  }
}
```

### Campos del config

| Campo | DescripciÃ³n |
|-------|-------------|
| `product` | Tipo de producto (salary, electricity, social_comments) |
| `query_template` | Template para construir la query de bÃºsqueda |
| `freshness` | Filtro temporal para Brave (`pw`=semana pasada, `py`=aÃ±o pasado) |
| `extraction_prompt` | Prompt especÃ­fico para extracciÃ³n de datos del dominio |
| `validation_prompt` | Prompt especÃ­fico para validaciÃ³n de datos |
| `countries` | Mapa de paÃ­ses con idioma y moneda |

---

## Structured Output

### Path NATIVO (recomendado) â€” 1 llamada LLM

Usa `response_model=` en el constructor. Internamente pasa `response_format=` a `create_agent`:

```python
from aifoundry.app.schemas.agent_responses import SalaryResponse

async with ScraperAgent(response_model=SalaryResponse) as agent:
    result = await agent.run(config)
    salary_data = result["structured_response"]  # SalaryResponse Pydantic object
```

### Path LEGACY (deprecated) â€” 2 llamadas LLM

Usa `structured_output=True`. Hace una segunda llamada LLM con `with_structured_output()`:

```python
# âš ï¸ DEPRECATED â€” emite DeprecationWarning
async with ScraperAgent(structured_output=True) as agent:
    result = await agent.run(config)
```

### Modelos disponibles

Definidos en `aifoundry/app/schemas/agent_responses.py`:

| Modelo | Producto | Campos principales |
|--------|----------|--------------------|
| `SalaryResponse` | `salary` | `salaries[]`, `sources[]`, `summary`, `confidence` |
| `ElectricityResponse` | `electricity` | (por definir) |
| `SocialCommentsResponse` | `social_comments` | (por definir) |

---

## Tools: Locales + MCP

### Tools MCP (Docker)

| Tool | Servidor MCP | Transport | DescripciÃ³n |
|------|-------------|-----------|-------------|
| `brave_web_search` | Brave Search | `streamable_http` | BÃºsqueda web con filtro de freshness |
| `browser_*` (navigate, snapshot, click, type, close) | Playwright | `streamable_http` | NavegaciÃ³n web completa |

### Tools Locales

| Tool | DescripciÃ³n |
|------|-------------|
| `simple_scrape_url` | Scraping rÃ¡pido con BeautifulSoup + Readability. Fallback antes de Playwright. |

### Nota sobre system_prompt

`create_agent` acepta `prompt=` pero es **estÃ¡tico** (se fija al crear el agente). Nuestro prompt es **dinÃ¡mico** (cambia por paÃ­s/producto/provider en cada `run()`), asÃ­ que lo pasamos como `SystemMessage` en los messages de cada invocaciÃ³n. Esto es un patrÃ³n vÃ¡lido documentado en la API oficial.

---

## Memoria Conversacional

El agente usa `InMemorySaver` de LangGraph como checkpointer:

```python
# Multi-turn: el agente recuerda entre llamadas
async with ScraperAgent(use_memory=True) as agent:
    r1 = await agent.run(config1)  # El agente recuerda esto...
    r2 = await agent.run(config2)  # ...cuando procesa esto
    
    history = agent.get_history()  # Lista de mensajes
    print(agent.thread_id)         # ID del thread
```

### Reset de memoria

```python
agent.reset_memory()  # Nuevo InMemorySaver + nuevo thread_id
```

---

## Crear un Nuevo Dominio

Para aÃ±adir un nuevo dominio (ej: `real_estate`):

### 1. Crear el config.json

```bash
mkdir -p aifoundry/app/core/agents/real_estate
```

```json
// aifoundry/app/core/agents/real_estate/config.json
{
  "product": "real_estate",
  "query_template": "precio alquiler {type} {city} {country_name} {date}",
  "freshness": "pm",
  "extraction_prompt": "Extrae precios de alquiler...",
  "validation_prompt": "Verifica que los precios sean realistas...",
  "countries": {
    "ES": { "language": "es", "currency": "EUR" }
  }
}
```

### 2. Crear el modelo de respuesta (opcional)

```python
# aifoundry/app/schemas/agent_responses.py â€” aÃ±adir:
class RealEstateResponse(BaseModel):
    city: str
    prices: List[PriceEntry]
    sources: List[str]
    summary: str

# AÃ±adir al mapping:
RESPONSE_MODELS["real_estate"] = RealEstateResponse
```

### 3. Usar desde la API

La API ya soporta cualquier producto que tenga `config.json`:

```bash
curl -X POST http://localhost:8000/api/scrape \
  -H "Content-Type: application/json" \
  -d '{"product": "real_estate", "provider": "Idealista", "country": "ES"}'
```

### 4. Test manual

```python
# scripts/test_real_estate_agent.py
from aifoundry.app.core.agents.scraper.agent import ScraperAgent
from aifoundry.app.schemas.agent_responses import RealEstateResponse

async with ScraperAgent(response_model=RealEstateResponse) as agent:
    result = await agent.run({
        "product": "real_estate",
        "provider": "Idealista",
        "country_code": "ES",
        "language": "es",
        "query": "precio alquiler piso Madrid EspaÃ±a 2026",
    })
```

**No se necesita crear clases nuevas ni registrar nada** â€” solo el `config.json` y opcionalmente un modelo Pydantic.

---

## API REST

### Endpoint principal

```
POST /api/scrape
```

```json
{
  "product": "salary",
  "provider": "BCG",
  "country": "ES",
  "position": "Lead Architect",
  "city": "Madrid"
}
```

La API (`router.py`) automÃ¡ticamente:
1. Carga el `config.json` del producto
2. Construye la query desde `query_template`
3. Instancia `ScraperAgent` con `response_model=` si existe modelo para el producto
4. Devuelve JSON estructurado o texto libre

---

## Troubleshooting

### El agente no encuentra datos

1. Verifica que Docker estÃ¡ corriendo (`docker-compose up -d`)
2. Verifica que los MCP servers responden: `curl http://localhost:8930/mcp` (Brave), `curl http://localhost:8931/mcp` (Playwright)
3. Revisa el `freshness` en config.json â€” `pw` (semana pasada) puede ser muy restrictivo

### Error de conexiÃ³n MCP

```
Error cargando MCP tools: Connection refused
```

Los MCP servers corren en Docker. Ejecuta:
```bash
docker-compose up -d
```

### Structured output vacÃ­o

Si `result["structured_response"]` es `None`:
1. El LLM no generÃ³ datos suficientes para llenar el schema
2. Se usa fallback automÃ¡tico a post-procesamiento (segunda llamada LLM)
3. Revisa los logs: busca `"structured_response no encontrado"`

### Logs de debug

```python
import logging
logging.basicConfig(level=logging.DEBUG)
logging.getLogger("aifoundry").setLevel(logging.DEBUG)
```

---

## Referencias

- [LangChain Docs - Agents](https://python.langchain.com/docs/concepts/agents/)
- [LangChain API - create_agent](https://python.langchain.com/api_reference/langchain/agents.html)
- [LangGraph](https://langchain-ai.github.io/langgraph/)
- [MCP Protocol](https://modelcontextprotocol.io/)