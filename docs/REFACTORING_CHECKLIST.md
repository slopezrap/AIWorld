# üîß Checklist de Refactoring ‚Äî AIFoundry Backend

> **Versi√≥n:** 1.0.0  
> **Fecha:** 2026-02-23  
> **Contexto:** Este checklist detalla las 4 grandes √°reas de refactoring identificadas en la auditor√≠a t√©cnica, dise√±adas para alinear el backend con la propuesta de frontend (`FRONTEND_DESIGN_PROPOSAL.md`) y preparar el sistema para producci√≥n.

---

## √çndice

1. [Refactorizar `agent.py`](#1-refactorizar-agentpy)
2. [A√±adir Autenticaci√≥n](#2-a√±adir-autenticaci√≥n)
3. [Implementar SSE/Streaming](#3-implementar-ssestreaming)
4. [Memoria Persistente](#4-memoria-persistente)

---

## 1. Refactorizar `agent.py`

> **Objetivo:** Descomponer el monolito `ConfigurableAgent` (~400 l√≠neas) en m√≥dulos cohesivos con responsabilidad √∫nica. Esto facilita testing, mantenimiento y la futura integraci√≥n de streaming.

### 1.1 An√°lisis y Preparaci√≥n

- [ ] **Mapear dependencias internas** del `ConfigurableAgent` actual:
  - Identificar los 5 bloques funcionales: memoria, tool calling, ejecuci√≥n de fases, parsing de output, orquestaci√≥n principal.
  - Documentar qu√© m√©todos llaman a qu√© otros para entender el acoplamiento.
- [ ] **Definir interfaces/contratos** (Protocol classes o ABCs) para cada m√≥dulo extra√≠do:
  - `MemoryManager` ‚Äî gesti√≥n de sesiones y mensajes
  - `ToolExecutor` ‚Äî resoluci√≥n y ejecuci√≥n de tools (MCP + internas)
  - `PhaseRunner` ‚Äî ejecuci√≥n secuencial de fases
  - `OutputParser` ‚Äî extracci√≥n de structured data del LLM
  - `AgentOrchestrator` ‚Äî coordinaci√≥n de todo lo anterior (el nuevo `agent.py` reducido)
- [ ] **Escribir tests del agente actual** (con LLM mockeado) ANTES de refactorizar, para tener red de seguridad:
  - Test de `chat()` con respuesta simple (sin tools)
  - Test de `chat()` con tool calling (mock de MCP)
  - Test de ejecuci√≥n multi-fase
  - Test de memoria multi-turno
  - Test de cleanup de sesiones expiradas

### 1.2 Extracci√≥n del M√≥dulo de Memoria (`memory.py`)

- [ ] Crear `aifoundry/app/core/agents/base/memory.py`
- [ ] Extraer de `agent.py`:
  - `conversation_memory: dict` ‚Üí clase `ConversationMemoryManager`
  - `cleanup_expired_sessions()` ‚Üí m√©todo de la clase
  - `_get_memory()` / `_add_to_memory()` ‚Üí m√©todos de la clase
  - Toda la l√≥gica de TTL y maxlen
- [ ] Definir interfaz abstracta `BaseMemoryManager` (Protocol):
  ```python
  class BaseMemoryManager(Protocol):
      async def get_messages(self, session_id: str) -> list[dict]
      async def add_message(self, session_id: str, role: str, content: str) -> None
      async def clear_session(self, session_id: str) -> None
      async def cleanup_expired(self) -> int  # retorna n¬∫ de sesiones eliminadas
  ```
- [ ] Implementar `InMemoryManager(BaseMemoryManager)` ‚Äî comportamiento actual
- [ ] **Reemplazar `cleanup_expired_sessions()` en cada request** por un `asyncio` background task peri√≥dico (cada 60s) registrado en el `lifespan` de FastAPI
- [ ] Actualizar `agent.py` para inyectar `MemoryManager` en el constructor
- [ ] Verificar que los tests existentes siguen pasando

### 1.3 Extracci√≥n del Tool Executor (`tool_executor.py`)

- [ ] Crear `aifoundry/app/core/agents/base/tool_executor.py`
- [ ] Extraer de `agent.py`:
  - `_get_available_tools()` ‚Üí resoluci√≥n de tools desde config (MCP refs + internas)
  - `_execute_tool_call()` ‚Üí ejecuci√≥n de una tool individual
  - `_connect_mcp_and_get_tools()` ‚Üí conexi√≥n a MCP servers
  - Mapping de herramientas internas (`simple_scrape_url`, `get_country_info`)
- [ ] Definir clase `ToolExecutor`:
  ```python
  class ToolExecutor:
      async def resolve_tools(self, tool_names: list[str]) -> list[ToolSchema]
      async def execute(self, tool_name: str, arguments: dict) -> ToolResult
      async def get_openai_tool_schemas(self, tool_names: list[str]) -> list[dict]
  ```
- [ ] **Implementar pool/cache de conexiones MCP** ‚Äî en lugar de abrir/cerrar por cada request:
  - Cache de `ClientSession` por URL de MCP server
  - Reconexi√≥n autom√°tica si la sesi√≥n se cierra
  - Timeout configurable para conexiones MCP
- [ ] Hacer `max_iterations` configurable desde `config.json` (actualmente hardcoded a 20)
- [ ] A√±adir **eventos/callbacks** en la ejecuci√≥n de tools para preparar el streaming:
  ```python
  class ToolEvent:
      event_type: Literal["tool_start", "tool_result", "tool_error"]
      tool_name: str
      arguments: dict | None
      result: str | None
  ```
- [ ] Actualizar `agent.py` para usar `ToolExecutor` inyectado
- [ ] Tests unitarios del `ToolExecutor` con MCP mockeado

### 1.4 Extracci√≥n del Phase Runner (`phase_runner.py`)

- [ ] Crear `aifoundry/app/core/agents/base/phase_runner.py`
- [ ] Extraer de `agent.py`:
  - Toda la l√≥gica del loop `for phase in phases` dentro de `chat()`
  - Construcci√≥n de prompts por fase (system prompt override, inyecci√≥n de output schema)
  - Acumulaci√≥n de resultados entre fases
- [ ] Definir clase `PhaseRunner`:
  ```python
  class PhaseRunner:
      def __init__(self, tool_executor: ToolExecutor, llm_client, output_parser: OutputParser)
      async def run_phases(self, phases: list[PhaseConfig], context: PhaseContext) -> PhaseResult
      async def run_single_phase(self, phase: PhaseConfig, context: PhaseContext) -> PhaseResult
  ```
- [ ] `PhaseContext` debe contener: messages previos, memoria de sesi√≥n, country, params, resultados de fases anteriores
- [ ] A√±adir **eventos/callbacks por fase** para streaming:
  ```python
  class PhaseEvent:
      event_type: Literal["phase_start", "phase_complete", "thinking"]
      phase_id: str
      content: str | None
  ```
- [ ] Tests unitarios del `PhaseRunner`

### 1.5 Extracci√≥n del Output Parser (`output_parser.py`)

- [ ] Crear `aifoundry/app/core/agents/base/output_parser.py`
- [ ] Extraer de `agent.py`:
  - `_parse_structured_output()` ‚Äî extracci√≥n de JSON del texto LLM
  - L√≥gica de bloques markdown (```json...```)
  - Validaci√≥n contra el `output_schema` del config
  - Fallback cuando el LLM no devuelve JSON v√°lido
- [ ] Definir clase `OutputParser`:
  ```python
  class OutputParser:
      def parse(self, raw_text: str, expected_schema: dict | None) -> ParsedOutput
      def extract_json_block(self, text: str) -> dict | None
      def validate_against_schema(self, data: dict, schema: dict) -> ValidationResult
  ```
- [ ] Tests unitarios con m√∫ltiples formatos de output del LLM (JSON limpio, markdown block, texto mixto, JSON inv√°lido)

### 1.6 Agente Orquestador Simplificado (`agent.py` refactorizado)

- [ ] Refactorizar `ConfigurableAgent` para que solo orqueste:
  ```python
  class ConfigurableAgent:
      def __init__(self, config: AgentConfig, memory: BaseMemoryManager, 
                   tool_executor: ToolExecutor, phase_runner: PhaseRunner, 
                   output_parser: OutputParser)
      async def chat(self, query: str, session_id: str, ...) -> ChatResult
  ```
- [ ] El m√©todo `chat()` simplificado deber√≠a ser ~50 l√≠neas m√°ximo:
  1. Recuperar memoria de sesi√≥n
  2. Construir contexto
  3. Ejecutar fases v√≠a `PhaseRunner`
  4. Parsear output v√≠a `OutputParser`
  5. Guardar en memoria
  6. Retornar resultado
- [ ] Crear **factory function** `create_agent(config_path: str) -> ConfigurableAgent` que ensamble todas las dependencias
- [ ] Verificar que TODOS los tests (nuevos y existentes) pasan
- [ ] Actualizar los imports en `router.py`

### 1.7 Actualizaci√≥n de Documentaci√≥n

- [ ] Actualizar `docs/AGENTS.md` con la nueva estructura modular
- [ ] Documentar las interfaces/protocolos para contribuidores
- [ ] Actualizar los diagramas de arquitectura si los hay

---

## 2. A√±adir Autenticaci√≥n

> **Objetivo:** Proteger los endpoints con API key, alineado con lo que el frontend necesitar√°. El `FRONTEND_DESIGN_PROPOSAL.md` asume comunicaci√≥n directa backend‚Üîfrontend, por lo que la auth debe ser ligera pero segura.

### 2.1 Dise√±o

- [ ] **Elegir mecanismo:** API Key en header `X-API-Key` (simple, suficiente para Phase 1 del frontend)
  - Futuro: OAuth2/JWT para multi-usuario (Phase 2+)
- [ ] **Definir configuraci√≥n:**
  ```env
  # .env
  API_KEYS=key1,key2,key3          # Lista de API keys v√°lidas (comma-separated)
  AUTH_ENABLED=true                  # Toggle para desarrollo local
  ```
- [ ] **Definir qu√© endpoints proteger:**
  - `GET /api/health` ‚Üí ‚ùå NO proteger (necesario para health checks de Docker/K8s)
  - `GET /api/agents` ‚Üí ‚úÖ Proteger
  - `GET /api/agents/{name}/config` ‚Üí ‚úÖ Proteger
  - `POST /api/agents/{name}/chat` ‚Üí ‚úÖ Proteger
  - `DELETE /api/agents/{name}/sessions/{id}` ‚Üí ‚úÖ Proteger
  - Futuro `GET /api/agents/{name}/stream` ‚Üí ‚úÖ Proteger

### 2.2 Implementaci√≥n

- [ ] A√±adir `api_keys` y `auth_enabled` a `config.py` (`Settings`):
  ```python
  api_keys: list[str] = []       # Si vac√≠o, auth deshabilitada
  auth_enabled: bool = False     # Toggle expl√≠cito
  ```
- [ ] Crear `aifoundry/app/api/auth.py`:
  ```python
  from fastapi import Security, HTTPException, status
  from fastapi.security import APIKeyHeader
  
  api_key_header = APIKeyHeader(name="X-API-Key", auto_error=False)
  
  async def verify_api_key(api_key: str = Security(api_key_header)) -> str:
      if not settings.auth_enabled:
          return "development"
      if api_key not in settings.api_keys:
          raise HTTPException(status_code=401, detail="API key inv√°lida o ausente")
      return api_key
  ```
- [ ] Aplicar como dependencia en `router.py`:
  ```python
  router = APIRouter(prefix="/api", dependencies=[Depends(verify_api_key)])
  ```
  - Excepto `/health` que debe estar fuera del router protegido o tener su propio router sin auth
- [ ] Actualizar `.env.example` con las nuevas variables
- [ ] Actualizar `docker-compose.yml` para pasar las variables de entorno

### 2.3 Testing

- [ ] Test: Request sin API key ‚Üí 401
- [ ] Test: Request con API key inv√°lida ‚Üí 401
- [ ] Test: Request con API key v√°lida ‚Üí 200
- [ ] Test: `/health` sin API key ‚Üí 200 (no protegido)
- [ ] Test: `auth_enabled=false` ‚Üí todas las requests pasan sin key

### 2.4 Documentaci√≥n

- [ ] Actualizar `README.md` con instrucciones de autenticaci√≥n
- [ ] Documentar en la secci√≥n de API de `AGENTS.md`
- [ ] A√±adir header `X-API-Key` a la documentaci√≥n OpenAPI (FastAPI lo har√° autom√°ticamente via `APIKeyHeader`)

---

## 3. Implementar SSE/Streaming

> **Objetivo:** Reemplazar el endpoint s√≠ncrono `POST /chat` con un sistema de streaming basado en SSE (Server-Sent Events), alineado con el **Phase 2** del `FRONTEND_DESIGN_PROPOSAL.md`. Esto elimina el cuello de botella del request bloqueante y habilita la UX de "pensamiento en vivo" tipo Cline.

### 3.1 Dise√±o del Protocolo SSE

> Referencia: `FRONTEND_DESIGN_PROPOSAL.md` secci√≥n "Streaming Protocol"

- [ ] **Definir tipos de evento SSE:**
  ```
  event: thinking        ‚Üí data: {"content": "Analizando la consulta..."}
  event: phase_start     ‚Üí data: {"phase_id": "search", "description": "Buscando informaci√≥n"}
  event: tool_start      ‚Üí data: {"tool": "brave_search", "arguments": {"query": "..."}}
  event: tool_result     ‚Üí data: {"tool": "brave_search", "result": "...", "success": true}
  event: text            ‚Üí data: {"content": "Fragmento de respuesta..."}  (token streaming)
  event: structured_data ‚Üí data: {"schema": "ElectricityResponse", "data": {...}}
  event: phase_complete  ‚Üí data: {"phase_id": "search", "summary": "..."}
  event: done            ‚Üí data: {"metadata": {"duration_ms": 1234, "model": "gpt-4o-mini", ...}}
  event: error           ‚Üí data: {"code": "LLM_ERROR", "message": "..."}
  ```
- [ ] **Definir schemas Pydantic** para cada tipo de evento en `aifoundry/app/api/schemas.py`:
  ```python
  class SSEEvent(BaseModel):
      event: str
      data: dict
  
  class ThinkingEvent(BaseModel): ...
  class ToolStartEvent(BaseModel): ...
  class ToolResultEvent(BaseModel): ...
  # etc.
  ```

### 3.2 Sistema de Eventos Interno

- [ ] Crear `aifoundry/app/core/agents/base/events.py`:
  - Definir `AgentEvent` (union type de todos los eventos posibles)
  - Definir `EventEmitter` ‚Äî interfaz para emitir eventos durante la ejecuci√≥n:
    ```python
    class EventEmitter(Protocol):
        async def emit(self, event: AgentEvent) -> None
    
    class AsyncQueueEmitter(EventEmitter):
        """Emite eventos a un asyncio.Queue para consumo por SSE."""
        def __init__(self):
            self.queue: asyncio.Queue[AgentEvent] = asyncio.Queue()
        
        async def emit(self, event: AgentEvent) -> None:
            await self.queue.put(event)
        
        async def events(self) -> AsyncGenerator[AgentEvent, None]:
            while True:
                event = await self.queue.get()
                if event.event == "done":
                    yield event
                    break
                yield event
    ```
- [ ] Integrar `EventEmitter` en los m√≥dulos refactorizados:
  - `ToolExecutor.execute()` ‚Üí emite `tool_start` y `tool_result`
  - `PhaseRunner.run_single_phase()` ‚Üí emite `phase_start`, `thinking`, `phase_complete`
  - `OutputParser.parse()` ‚Üí emite `structured_data`
  - `ConfigurableAgent.chat()` ‚Üí emite `done` o `error`

### 3.3 Endpoint SSE

- [ ] Crear nuevo endpoint `POST /api/agents/{name}/stream` en `router.py`:
  ```python
  from sse_starlette.sse import EventSourceResponse
  
  @router.post("/agents/{agent_name}/stream")
  async def stream_agent(agent_name: str, request: ChatRequest):
      emitter = AsyncQueueEmitter()
      
      # Lanzar ejecuci√≥n del agente en background
      async def run_agent():
          try:
              await agent.chat(query=request.query, ..., event_emitter=emitter)
          except Exception as e:
              await emitter.emit(ErrorEvent(message=str(e)))
      
      asyncio.create_task(run_agent())
      
      # Devolver stream SSE
      async def event_generator():
          async for event in emitter.events():
              yield {"event": event.event, "data": event.model_dump_json()}
      
      return EventSourceResponse(event_generator())
  ```
- [ ] A√±adir dependencia `sse-starlette` al `pyproject.toml`
- [ ] **Mantener el endpoint `/chat` s√≠ncrono** como fallback para clientes simples (scripts, testing)
- [ ] Implementar timeout global para el stream (ej: 5 minutos m√°ximo)

### 3.4 Token Streaming del LLM

- [ ] Modificar `llm.py` para soportar streaming:
  ```python
  async def acompletion_stream(self, messages, **kwargs) -> AsyncGenerator[str, None]:
      response = await litellm.acompletion(messages=messages, stream=True, **kwargs)
      async for chunk in response:
          delta = chunk.choices[0].delta.content
          if delta:
              yield delta
  ```
- [ ] Integrar con `PhaseRunner` para emitir eventos `text` por cada chunk
- [ ] **Nota:** Cuando hay tool calling, el LLM no streama texto sino tool calls. Manejar ambos flujos.

### 3.5 Compatibilidad con FRONTEND_DESIGN_PROPOSAL.md

- [ ] Verificar que los nombres de eventos SSE coinciden con los esperados por el frontend:
  - `thinking`, `tool_start`, `tool_result`, `text`, `structured_data`, `done`
- [ ] El endpoint debe aceptar el mismo `ChatRequest` body que `/chat`
- [ ] El evento `done` debe incluir los mismos `metadata` que la respuesta s√≠ncrona actual
- [ ] Verificar CORS headers para SSE (`text/event-stream`)

### 3.6 Testing

- [ ] Test unitario: `AsyncQueueEmitter` emite y consume eventos correctamente
- [ ] Test de integraci√≥n: `/stream` devuelve `Content-Type: text/event-stream`
- [ ] Test de integraci√≥n: El stream emite al menos `thinking` ‚Üí `done`
- [ ] Test de error: Si el agente falla, se emite evento `error` y el stream se cierra
- [ ] Test de timeout: Si el agente excede el timeout, se emite error y se cierra
- [ ] **Load test b√°sico:** 10 requests concurrentes de streaming no crashean el servidor

### 3.7 Documentaci√≥n

- [ ] Actualizar `FRONTEND_DESIGN_PROPOSAL.md` para reflejar el endpoint real (`/stream` en lugar de `/run`)
- [ ] Documentar el protocolo SSE en un nuevo `docs/STREAMING.md`
- [ ] A√±adir ejemplos de consumo SSE con `curl` y `EventSource` (JS)

---

## 4. Memoria Persistente

> **Objetivo:** Reemplazar el `dict` en memoria por un almacenamiento persistente que sobreviva reinicios y soporte escalado horizontal. El `FRONTEND_DESIGN_PROPOSAL.md` asume sesiones multi-turno, por lo que la persistencia es cr√≠tica.

### 4.1 Evaluaci√≥n de Opciones

- [ ] **Evaluar y decidir** entre las opciones:

  | Opci√≥n | Pros | Contras | Recomendada para |
  |--------|------|---------|-----------------|
  | **SQLite** | Sin infra extra, persistente, incluido en Python | No escala horizontal, lock en escritura | Desarrollo / Single instance |
  | **Redis** | R√°pido, TTL nativo, escala horizontal | Requiere infra extra (Docker service) | Producci√≥n / Multi-instance |
  | **PostgreSQL** | Robusto, transaccional, ya est√°ndar en empresas | M√°s complejo, overhead para K/V simple | Si ya existe en la infra |

  **Recomendaci√≥n:** Implementar **Redis como primary** (producci√≥n) con **SQLite como fallback** (desarrollo sin Docker).

### 4.2 Interfaz Abstracta (ya definida en 1.2)

- [ ] Confirmar que `BaseMemoryManager` del refactoring del agente soporta las operaciones necesarias:
  ```python
  class BaseMemoryManager(Protocol):
      async def get_messages(self, session_id: str) -> list[dict]
      async def add_message(self, session_id: str, role: str, content: str) -> None
      async def clear_session(self, session_id: str) -> None
      async def cleanup_expired(self) -> int
      async def list_sessions(self, agent_id: str) -> list[SessionInfo]  # NUEVO
      async def get_session_metadata(self, session_id: str) -> SessionMetadata | None  # NUEVO
  ```
- [ ] Definir `SessionInfo` y `SessionMetadata`:
  ```python
  class SessionMetadata(BaseModel):
      session_id: str
      agent_id: str
      created_at: datetime
      last_active: datetime
      message_count: int
      ttl_minutes: int
  ```

### 4.3 Implementaci√≥n Redis (`redis_memory.py`)

- [ ] Crear `aifoundry/app/core/agents/base/redis_memory.py`
- [ ] Implementar `RedisMemoryManager(BaseMemoryManager)`:
  - **Estructura de datos Redis:**
    ```
    session:{session_id}:messages  ‚Üí LIST de JSON strings (cada mensaje)
    session:{session_id}:metadata  ‚Üí HASH (agent_id, created_at, last_active, ttl)
    agent:{agent_id}:sessions      ‚Üí SET de session_ids
    ```
  - TTL nativo de Redis en las keys de sesi√≥n
  - `LTRIM` para respetar `max_messages`
- [ ] A√±adir dependencia `redis[hiredis]` al `pyproject.toml`
- [ ] A√±adir configuraci√≥n a `config.py`:
  ```python
  redis_url: str = "redis://localhost:6379/0"
  memory_backend: Literal["memory", "redis", "sqlite"] = "memory"
  ```
- [ ] A√±adir servicio Redis al `docker-compose.yml`:
  ```yaml
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
  ```

### 4.4 Implementaci√≥n SQLite (`sqlite_memory.py`)

- [ ] Crear `aifoundry/app/core/agents/base/sqlite_memory.py`
- [ ] Implementar `SQLiteMemoryManager(BaseMemoryManager)`:
  - **Schema:**
    ```sql
    CREATE TABLE sessions (
        session_id TEXT PRIMARY KEY,
        agent_id TEXT NOT NULL,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        last_active TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        ttl_minutes INTEGER DEFAULT 30
    );
    CREATE TABLE messages (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        session_id TEXT NOT NULL REFERENCES sessions(session_id),
        role TEXT NOT NULL,
        content TEXT NOT NULL,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );
    CREATE INDEX idx_messages_session ON messages(session_id);
    ```
  - Usar `aiosqlite` para operaciones async
  - Cleanup de sesiones expiradas via `DELETE WHERE last_active + ttl < now()`
- [ ] A√±adir dependencia `aiosqlite` al `pyproject.toml`
- [ ] DB file en ruta configurable: `sqlite_db_path: str = "data/memory.db"`

### 4.5 Factory de Memoria

- [ ] Crear factory en `aifoundry/app/core/agents/base/memory.py`:
  ```python
  def create_memory_manager(backend: str, config: Settings) -> BaseMemoryManager:
      match backend:
          case "memory":
              return InMemoryManager()
          case "redis":
              return RedisMemoryManager(redis_url=config.redis_url)
          case "sqlite":
              return SQLiteMemoryManager(db_path=config.sqlite_db_path)
          case _:
              raise ValueError(f"Backend de memoria no soportado: {backend}")
  ```
- [ ] Instanciar en el `lifespan` de `main.py` y compartir v√≠a `app.state`
- [ ] Inyectar en los agentes al crearlos

### 4.6 Migraci√≥n de Datos

- [ ] El `InMemoryManager` existente debe seguir funcionando sin cambios (backward compatible)
- [ ] A√±adir script de migraci√≥n `scripts/migrate_memory.py` (por si en futuro se cambia de backend)
- [ ] Documentar el proceso de cambio de backend

### 4.7 Background Tasks

- [ ] Registrar tarea peri√≥dica de cleanup en el `lifespan`:
  ```python
  async def periodic_cleanup(memory: BaseMemoryManager):
      while True:
          await asyncio.sleep(60)  # cada minuto
          cleaned = await memory.cleanup_expired()
          if cleaned > 0:
              logger.info(f"üßπ Cleaned {cleaned} expired sessions")
  ```
- [ ] La tarea debe cancelarse limpiamente en el shutdown

### 4.8 Testing

- [ ] Test unitario: `InMemoryManager` ‚Äî CRUD de mensajes, TTL, cleanup
- [ ] Test unitario: `RedisMemoryManager` ‚Äî usar `fakeredis` para mock
- [ ] Test unitario: `SQLiteMemoryManager` ‚Äî usar DB temporal en `/tmp`
- [ ] Test de integraci√≥n: Reiniciar servidor ‚Üí las sesiones persisten (Redis/SQLite)
- [ ] Test de integraci√≥n: M√∫ltiples workers comparten la misma memoria (Redis)
- [ ] Test de concurrencia: Escrituras simult√°neas no corrompen datos

### 4.9 Documentaci√≥n

- [ ] Actualizar `README.md` con opciones de backend de memoria
- [ ] Actualizar `docker-compose.yml` documentation
- [ ] Documentar variables de entorno nuevas en `.env.example`
- [ ] Actualizar `FRONTEND_DESIGN_PROPOSAL.md` secci√≥n de sesiones para reflejar la persistencia

---

## üìã Orden de Ejecuci√≥n Recomendado

```
Fase 1 (Fundamentos):
  1.1-1.7  Refactorizar agent.py          ‚Üê Primero, porque todo lo dem√°s depende de esto
  
Fase 2 (Seguridad + Persistencia, en paralelo):
  2.1-2.4  Autenticaci√≥n                  ‚Üê Independiente, se puede hacer en paralelo
  4.1-4.9  Memoria Persistente            ‚Üê Depende del refactoring de memoria (1.2)
  
Fase 3 (Streaming):
  3.1-3.7  SSE/Streaming                  ‚Üê Depende del sistema de eventos (1.3, 1.4)
```

### Dependencias entre tareas:

```
[1. Refactoring agent.py]
    ‚îú‚îÄ‚îÄ [1.2 Memoria] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí [4. Memoria Persistente]
    ‚îú‚îÄ‚îÄ [1.3 ToolExecutor] ‚îÄ‚îÄ‚Üí [3. SSE/Streaming (eventos de tools)]
    ‚îú‚îÄ‚îÄ [1.4 PhaseRunner] ‚îÄ‚îÄ‚îÄ‚Üí [3. SSE/Streaming (eventos de fases)]
    ‚îî‚îÄ‚îÄ [1.6 Orquestador] ‚îÄ‚îÄ‚Üí [3. SSE/Streaming (endpoint)]

[2. Autenticaci√≥n] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí (independiente, se aplica a todos los endpoints)
```

---

## üìä M√©tricas de √âxito

| M√©trica | Antes | Despu√©s |
|---------|-------|---------|
| L√≠neas en `agent.py` | ~400 | ~80 |
| M√≥dulos en `base/` | 4 archivos | 8+ archivos |
| Test coverage del agente | 0% | >80% |
| Tiempo de respuesta API | Bloqueante (30s+) | Primer evento SSE <1s |
| Sesiones tras reinicio | Perdidas | Persistentes |
| Endpoints protegidos | 0 | Todos (excepto health) |