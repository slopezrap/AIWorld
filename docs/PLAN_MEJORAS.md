# ðŸ“‹ Plan de Mejoras - AIFoundry

> RevisiÃ³n tÃ©cnica del proyecto contra la documentaciÃ³n oficial de LangChain (feb 2026) y buenas prÃ¡cticas.
> Fecha: 18 febrero 2026

---

## ðŸ” AuditorÃ­a del Estado Actual

### âœ… Lo que cumple con la documentaciÃ³n oficial

| Componente | Estado | Nota |
|------------|--------|------|
| `create_agent` de `langchain.agents` | âœ… Correcto | Es el API recomendado actualmente. Internamente crea un grafo LangGraph |
| `@tool` decorator para tools locales | âœ… Correcto | `tools.py` usa `@tool` de `langchain_core.tools`, forma estÃ¡ndar |
| `init_chat_model` para LLM factory | âœ… Correcto | `llm.py` usa `init_chat_model` con `openai` provider, forma recomendada |
| `MultiServerMCPClient` para MCP | âœ… Correcto | IntegraciÃ³n correcta con `langchain-mcp-adapters` |
| `InMemorySaver` como checkpointer | âœ… Correcto | PatrÃ³n estÃ¡ndar de LangGraph para persistencia de estado |
| `SystemMessage` + `HumanMessage` | âœ… Correcto | Forma estÃ¡ndar de construir mensajes |
| Callbacks (`BaseCallbackHandler`) | âœ… Correcto | `AgentCallbackHandler` implementa correctamente el patrÃ³n de callbacks |
| Structured output con Pydantic | âœ… Correcto | Usa `with_structured_output()` como post-procesamiento |
| PatrÃ³n ReAct | âœ… Correcto | `create_agent` implementa ReAct por defecto |

### âš ï¸ Lo que necesita mejora

| Componente | Problema | Referencia docs |
|------------|----------|-----------------|
| FastAPI sin endpoints | App creada pero sin rutas REST para agentes | FastAPI best practices |
| Sin validaciÃ³n de configs | `config.json` se carga con `json.load()` sin validar schema | Pydantic BaseSettings |
| Sin retry en LLM calls | Si `litellm` falla no hay backoff exponencial | `tenacity` (ya en deps) |
| Prompts hardcodeados en cÃ³digo | `prompts.py` tiene la estructura fija, no viene del config | LangChain PromptTemplate |
| Sin tests unitarios | `aifoundry/tests/` estÃ¡ vacÃ­o | pytest + pytest-asyncio |
| `response_format` no usado en `create_agent` | Se hace post-proceso manual en vez de usar el parÃ¡metro nativo | `create_agent(response_format=...)` |

---

## ðŸ›  Plan de ImplementaciÃ³n

### Tarea 1: Completar FastAPI con Endpoints REST (ðŸ”´ Alta)

**Problema**: `main.py` tiene FastAPI configurado (CORS, lifespan) pero sin ningÃºn endpoint. Los agentes solo se ejecutan desde scripts manuales.

**SoluciÃ³n**: Crear un router `api/agents.py` con endpoint genÃ©rico que use el `AgentFactory` existente.

**Archivos a crear/modificar**:
- [ ] Crear `aifoundry/app/api/__init__.py`
- [ ] Crear `aifoundry/app/api/agents.py` â€” Router con endpoints
- [ ] Crear `aifoundry/app/api/schemas.py` â€” Request/Response models para la API
- [ ] Modificar `aifoundry/app/main.py` â€” Registrar el router

**Endpoint principal**:
```
POST /api/agents/{agent_name}/run
```

**Request body**:
```json
{
  "provider": "Endesa",
  "country": "ES",
  "config_overrides": {}  // opcional
}
```

**Response**:
```json
{
  "agent": "electricity",
  "status": "success",
  "data": { ... },  // structured output del agente
  "metadata": {
    "execution_time_seconds": 12.5,
    "steps_count": 7
  }
}
```

**Endpoints adicionales**:
```
GET  /api/agents              â€” Lista agentes disponibles
GET  /api/agents/{name}/config â€” Devuelve config del agente
GET  /health                   â€” Health check
```

**Buenas prÃ¡cticas aplicadas**:
- Usar `APIRouter` con prefijo `/api`
- Modelos Pydantic para request/response
- Manejo de errores con `HTTPException`
- Timeout configurable para ejecuciÃ³n de agentes
- Logging estructurado

---

### Tarea 2: Tests Unitarios con pytest (ðŸ”´ Alta)

**Problema**: Directorio `aifoundry/tests/` vacÃ­o. Solo hay scripts manuales en `scripts/`.

**SoluciÃ³n**: Crear suite de tests unitarios con pytest + pytest-asyncio (ya estÃ¡n en dependencies).

**Archivos a crear**:
- [ ] `aifoundry/tests/conftest.py` â€” Fixtures compartidos
- [ ] `aifoundry/tests/test_prompts.py` â€” Tests de generaciÃ³n de prompts
- [ ] `aifoundry/tests/test_config_validation.py` â€” Tests de validaciÃ³n de configs
- [ ] `aifoundry/tests/test_utils_text.py` â€” Tests de `extract_json`, `clean_text`
- [ ] `aifoundry/tests/test_utils_country.py` â€” Tests de `get_country_info`, `get_brave_config`
- [ ] `aifoundry/tests/test_rate_limiter.py` â€” Tests del `AsyncRateLimiter`
- [ ] `aifoundry/tests/test_api.py` â€” Tests de endpoints con `httpx.AsyncClient`

**QuÃ© testear**:

| MÃ³dulo | Tests |
|--------|-------|
| `prompts.py` | GeneraciÃ³n correcta de system prompt con distintas configs, fecha en espaÃ±ol, traducciÃ³n de idiomas |
| `text.py` | `extract_json` con JSON vÃ¡lido, invÃ¡lido, embebido en texto, array vs object |
| `country.py` | `get_country_info` para paÃ­ses existentes y no existentes, `get_brave_config` |
| `rate_limiter.py` | Que respete el rate limit, que no bloquee bajo el lÃ­mite |
| `agent_responses.py` | Que `get_response_schema` devuelva el schema correcto por nombre |
| `api/agents.py` | Endpoints responden correctamente, errores 404 para agentes inexistentes |
| `AgentConfig` (nuevo) | ValidaciÃ³n de configs vÃ¡lidos e invÃ¡lidos |

**Buenas prÃ¡cticas**:
- Fixtures para configs de ejemplo
- Mocking de llamadas a LLM (no tests de integraciÃ³n costosos)
- `pytest.mark.asyncio` para tests async
- Cobertura mÃ­nima objetivo: utils y prompts al 90%

---

### Tarea 3: Validar config.json con Pydantic (ðŸŸ¡ Media)

**Problema**: Los `config.json` se cargan con `json.load()` sin validaciÃ³n. Si falta un campo o tiene un tipo incorrecto, el error aparece mucho despuÃ©s en la ejecuciÃ³n.

**SoluciÃ³n**: Crear modelo `AgentConfig` con Pydantic que valide al cargar.

**Archivos a crear/modificar**:
- [ ] Crear `aifoundry/app/core/agents/base/config.py` â€” Modelo `AgentConfig`
- [ ] Modificar `aifoundry/app/core/agents/base/agent.py` â€” Usar `AgentConfig` al cargar config

**Modelo propuesto**:
```python
from pydantic import BaseModel, Field
from typing import Dict, List, Optional

class CountryConfig(BaseModel):
    providers: List[str] = Field(..., min_length=1)
    language: str = Field(..., pattern=r'^[a-z]{2}$')

class AgentConfig(BaseModel):
    product: str = Field(..., min_length=1)
    freshness: str = Field(default="pw")
    query_template: str = Field(...)
    countries: Dict[str, CountryConfig]
    extraction_prompt: Optional[str] = None
    validation_prompt: Optional[str] = None
    response_schema: Optional[Dict] = None

    @classmethod
    def from_json(cls, path: str) -> "AgentConfig":
        import json
        with open(path) as f:
            return cls(**json.load(f))
```

**Beneficios**:
- Errores claros al cargar: `"field 'product' is required"`
- Autocompletado en IDE
- DocumentaciÃ³n implÃ­cita del schema
- SerializaciÃ³n/deserializaciÃ³n gratis

---

### Tarea 4: Mover Templates de Prompts al Config (ðŸŸ¡ Media)

**Problema**: `prompts.py` tiene una estructura fija de 7 pasos hardcodeada. Los Ãºnicos campos que vienen del config son `extraction_prompt` y `validation_prompt` (pasos 6 y 7). El resto del prompt es idÃ©ntico para todos los agentes.

**AnÃ¡lisis**: Para el caso actual (todos los agentes son "scrapers web"), la estructura de 7 pasos compartida tiene sentido. Pero si quisieras un agente que NO haga scraping (ej: un agente de RAG o un chatbot), no podrÃ­a reusar esos prompts.

**SoluciÃ³n**: Hacer el system prompt configurable por agente, manteniendo un default para los scrapers.

**Archivos a modificar**:
- [ ] Modificar configs JSON â€” AÃ±adir campo opcional `system_prompt_template`
- [ ] Modificar `aifoundry/app/core/agents/base/prompts.py` â€” Soportar template personalizado
- [ ] Modificar `aifoundry/app/core/agents/base/config.py` â€” AÃ±adir campo al modelo

**Estrategia**:
```json
{
  "product": "electricidad",
  "system_prompt_template": null,  // null = usa el default de 7 pasos
  "extraction_prompt": "...",      // personaliza solo paso 6
  "validation_prompt": "..."       // personaliza solo paso 7
}
```

Si `system_prompt_template` es `null`, se usa el prompt de 7 pasos actual (retrocompatible). Si se proporciona, se usa como template con variables `{product}`, `{provider}`, `{country}`, `{date}`, etc.

---

### Tarea 5: Retry con Backoff Exponencial (ðŸŸ¡ Media)

**Problema**: En `agent.py`, las llamadas al LLM se hacen sin retry. Si hay un error de red o rate limit, falla directamente. Aunque hay un `max_retries` en el agente, solo cubre errores muy especÃ­ficos de scraping, no errores de la API del LLM.

**SoluciÃ³n**: Usar `tenacity` (ya estÃ¡ en `pyproject.toml`) para retry con backoff exponencial en `get_llm()` o en el wrapper del agente.

**Archivos a modificar**:
- [ ] Modificar `aifoundry/app/core/models/llm.py` â€” AÃ±adir retry config al modelo
- [ ] Modificar `aifoundry/app/core/agents/base/agent.py` â€” Wrap de invocaciÃ³n con retry

**ImplementaciÃ³n**:
```python
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import httpx

# En llm.py - configurar max_retries del modelo
def get_llm() -> BaseChatModel:
    ...
    _llm_instance = init_chat_model(
        settings.litellm_model,
        model_provider="openai",
        temperature=settings.litellm_temperature,
        max_retries=3,  # <-- AÃ±adir retry nativo de LangChain
        request_timeout=120,  # <-- Timeout explÃ­cito
        ...
    )
```

**Nota**: `init_chat_model` con provider `openai` ya soporta `max_retries` de forma nativa a nivel de llamada HTTP. Esto es mÃ¡s limpio que un wrapper con tenacity porque maneja los retry-after headers automÃ¡ticamente.

---

## ðŸ“Š Orden de ImplementaciÃ³n

```
Tarea 3 (Pydantic config)     â† Base para todo lo demÃ¡s
    â†“
Tarea 4 (Prompts al config)   â† Depende de tener AgentConfig
    â†“
Tarea 5 (Retry LLM)           â† Independiente, rÃ¡pido
    â†“
Tarea 1 (FastAPI endpoints)   â† Usa AgentConfig para listar/validar
    â†“
Tarea 2 (Tests)               â† Testea todo lo anterior
```

---

## âœ… Checklist de EjecuciÃ³n

- [ ] **Tarea 3**: Crear `AgentConfig` Pydantic + integrar en `agent.py`
- [ ] **Tarea 4**: System prompt configurable por agente
- [ ] **Tarea 5**: Retry con `max_retries` + timeout en `get_llm()`
- [ ] **Tarea 1**: Router FastAPI con endpoints REST
- [ ] **Tarea 2**: Tests unitarios con pytest
- [ ] Verificar que todos los scripts de test siguen funcionando
- [ ] Actualizar `README.md` con los nuevos endpoints
- [ ] Actualizar `docs/AGENTS.md` con la nueva configuraciÃ³n

---

## ðŸ”‘ Nota sobre `response_format` en `create_agent`

La documentaciÃ³n actual de LangChain muestra que `create_agent` acepta `response_format` como parÃ¡metro para forzar structured output. Tu cÃ³digo actual hace un post-procesamiento con `_convert_to_structured()` usando `llm.with_structured_output()`. 

**RecomendaciÃ³n**: Mantener tu enfoque actual de post-procesamiento. El `response_format` nativo de `create_agent` tiene limitaciones documentadas:
1. No todos los modelos lo soportan
2. Puede interferir con tool calling en algunos providers
3. Tu approach de "dejar que el agente trabaje libre y luego estructurar" es mÃ¡s robusto

Esto NO es un problema a arreglar, es una decisiÃ³n de diseÃ±o correcta.